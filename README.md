Sharing Curated List of Machine learning papers to get you started in the world of Machine Learning Research.

**If you want to collaborate in the repo, reach me @ [Mail](mailto:sujaykapadnis.33@gmail.com?subject=Contribution_To_Research_Papers)**
# Intro to Neural Networks

## 1. Introduction to Neural Networks
- [Deep Learning in Neural Networks: An Overview](https://arxiv.org/pdf/1404.7828.pdf)
- [An overview of gradient descent optimization algorithms](https://arxiv.org/pdf/1609.04747.pdf)

## 2. Foundation Models and its Applications
- [On the Opportunities and Risks of Foundation Models](https://arxiv.org/abs/2108.07258)
- [Multimodal Foundation Models: From Specialists to General-Purpose Assistants](https://arxiv.org/pdf/2309.10020.pdf)
- [Large Multimodal Models: Notes on CVPR 2023 Tutorial](https://arxiv.org/pdf/2306.14895.pdf)
- [Towards Generalist Biomedical AI](https://arxiv.org/pdf/2307.14334.pdf)
- [A Comprehensive Survey on Pretrained Foundation Models: A History from BERT to ChatGPT](https://arxiv.org/pdf/2302.09419.pdf)
- [Interactive Natural Language Processing](https://arxiv.org/pdf/2305.13246.pdf)
- [Towards Reasoning in Large Language Models: A Survey](https://arxiv.org/pdf/2212.10403.pdf)


## 3. RNNs and CNNs
- [Recurrent Neural Networks (RNNs): A gentle Introduction and Overview](https://arxiv.org/pdf/1912.05911.pdf)
- [Long Short-Term Memory](https://www.bioinf.jku.at/publications/older/2604.pdf)
- [Empirical Evaluation of Gated Recurrent Neural Networks on Sequence Modeling](https://arxiv.org/pdf/1412.3555.pdf)
- [Highway Networks](https://arxiv.org/pdf/1505.00387.pdf)
- [Recursive Deep Models for Semantic Compositionality Over a Sentiment Treebank](https://aclanthology.org/D13-1170.pdf)
- [Sequence to Sequence Learning with Neural Networks](https://arxiv.org/pdf/1409.3215.pdf)
- [ImageNet Classification with Deep Convolutional Neural Networks](https://proceedings.neurips.cc/paper_files/paper/2012/file/c399862d3b9d6b76c8436e924a68c45b-Paper.pdf)
- [An Introduction to Convolutional Neural Networks](https://arxiv.org/pdf/1511.08458.pdf)
- [U-Net: Convolutional Networks for Biomedical Image Segmentation](https://arxiv.org/pdf/1505.04597.pdf)
- [Deep Residual Learning for Image Recognition](https://arxiv.org/pdf/1512.03385.pdf)
- [Densely Connected Convolutional Networks](https://arxiv.org/pdf/1608.06993.pdf)
- [Aggregated Residual Transformations for Deep Neural Networks](https://arxiv.org/pdf/1611.05431.pdf)

## 4. NLP and CV
- [Sequence to Sequence Learning with Neural Networks](https://arxiv.org/pdf/1409.3215.pdf)
- [Thumbs up? Sentiment Classification using Machine Learning Techniques](https://aclanthology.org/W02-1011.pdf)
- [A survey of named entity recognition and classification](https://nlp.cs.nyu.edu/sekine/papers/li07.pdf)
- [Teaching Machines to Read and Comprehend](https://arxiv.org/pdf/1506.03340.pdf)
- [Deep neural networks for acoustic modeling in speech recognition](https://static.googleusercontent.com/media/research.google.com/en//pubs/archive/38131.pdf)
- [A Neural Attention Model for Sentence Summarization](https://arxiv.org/pdf/1509.00685.pdf)
- [Microsoft COCO: Common Objects in Context](https://arxiv.org/pdf/1405.0312.pdf)
- [Rich feature hierarchies for accurate object detection and semantic segmentationFully Convolutional Networks for Semantic SegmentationDeepFace: Closing the Gap to Human-Level Performance in Face VerificationDeepPose: Human Pose Estimation via Deep Neural Networks](https://arxiv.org/pdf/1311.2524.pdf)
- [Fully Convolutional Networks for Semantic Segmentation](https://arxiv.org/pdf/1411.4038.pdf)
- [DeepFace: Closing the Gap to Human-Level Performance in Face Verification](https://www.cs.toronto.edu/~ranzato/publications/taigman_cvpr14.pdf)
- [DeepPose: Human Pose Estimation via Deep Neural Networks](https://ieeexplore.ieee.org/document/6909610)



# Transformer Architecture

## 1. Self Attention & Transformers
- [Neural Machine Translation by Jointly Learning to Align and Translate](https://arxiv.org/pdf/1409.0473.pdf)
- [Show, Attend and Tell: Neural Image Caption Generation with Visual Attention](https://proceedings.mlr.press/v37/xuc15.html)
- [Attention-Based Models for Speech Recognition](https://proceedings.neurips.cc/paper_files/paper/2015/file/1068c6e4c8051cfd4e9ea8072e3189e2-Paper.pdf)
- [Attention Is All You Need](https://arxiv.org/pdf/1706.03762.pdf)

## 2. Visual Transformers
- [An Image is Worth 16x16 Words: Transformers for Image Recognition at Scale](https://openreview.net/forum?id=YicbFdNTTy)
- [Swin Transformer: Hierarchical Vision Transformer using Shifted Windows](https://arxiv.org/pdf/2103.14030.pdf)
- [EfficientNet: Rethinking Model Scaling for Convolutional Neural Networks](https://arxiv.org/pdf/1905.11946.pdf)
- [Training data-efficient image transformers & distillation through attention](https://arxiv.org/pdf/2012.12877.pdf)

## 3. Efficient Transformers
- [Transformers are RNNs: Fast Autoregressive Transformers with Linear Attention](https://arxiv.org/pdf/2006.16236.pdf)
- [FNet: Mixing Tokens with Fourier Transforms](https://arxiv.org/pdf/2105.03824.pdf)
- [Random Feature Attention](https://arxiv.org/pdf/2103.02143.pdf)
- [ETC: Encoding Long and Structured Inputs in Transformers](https://arxiv.org/pdf/2004.08483.pdf)
- [Longformer: The Long-Document Transformer](https://arxiv.org/pdf/2004.05150.pdf)
- [Generating Long Sequences with Sparse TransformersLinformer: Self-Attention with Linear Complexity](https://arxiv.org/pdf/1904.10509.pdf)
- [Linformer: Self-Attention with Linear Complexity](https://arxiv.org/pdf/2006.04768.pdf)

## 4. Parameter-efficient Tuning
- [Parameter-Efficient Transfer Learning for NLP](https://arxiv.org/pdf/1902.00751.pdf)
- [LoRA: Low-Rank Adaptation of Large Language Models](https://arxiv.org/pdf/2106.09685.pdf)
- [The Power of Scale for Parameter-Efficient Prompt Tuning](https://arxiv.org/pdf/2104.08691.pdf)
- [It's Not Just Size That Matters: Small Language Models Are Also Few-Shot Learners](https://arxiv.org/pdf/2009.07118.pdf)
- [Making Pre-trained Language Models Better Few-shot Learners](https://aclanthology.org/2021.acl-long.295.pdf)

## 5. Language Model Pretraining
- [Deep contextualized word representations](https://arxiv.org/pdf/2108.12409.pdf)
- [BERT: Pre-training of Deep Bidirectional Transformers for Language Understanding](https://arxiv.org/pdf/1810.04805.pdf)
- [Unified Language Model Pre-training for Natural Language Understanding and Generation](https://arxiv.org/pdf/1905.03197.pdf)
- [ALBERT: A Lite BERT for Self-supervised Learning of Language Representations](https://arxiv.org/pdf/1909.11942.pdf)
- [RoBERTa: A Robustly Optimized BERT Pretraining Approach](https://arxiv.org/pdf/1907.11692.pdf)
- [ERNIE: Enhanced Language Representation with Informative Entities](https://arxiv.org/pdf/1905.07129.pdf)
- [ELECTRA: Pre-training Text Encoders as Discriminators Rather Than Generators](https://arxiv.org/pdf/2003.10555.pdf)

Curated List of Research Papers[Large Language Models]

## Large Language Models
- [Exploring the Limits of Transfer Learning with a Unified Text-to-Text Transformer](https://arxiv.org/pdf/1910.10683.pdf)
- [Language Models are Few-Shot Learners](https://arxiv.org/pdf/2005.14165.pdf)
- [Language Models are Unsupervised Multitask Learners](https://d4mucfpksywv.cloudfront.net/better-language-models/language_models_are_unsupervised_multitask_learners.pdf)
- [Evaluating Large Language Models Trained on Code](https://arxiv.org/pdf/2107.03374.pdf)
- [PaLM: Scaling Language Modeling with Pathways](https://arxiv.org/pdf/2204.02311.pdf)
- [Llama 2: Open Foundation and Fine-Tuned Chat Models](https://arxiv.org/pdf/2307.09288.pdf)

## Conditional Computation
- [Outrageously Large Neural Networks: The Sparsely-Gated Mixture-of-Experts Layer](https://arxiv.org/pdf/1701.06538.pdf)
- [GShard: Scaling Giant Models with Conditional Computation and Automatic Sharding](https://arxiv.org/pdf/2006.16668.pdf)
- [CoLT5: Faster Long-Range Transformers with Conditional Computation](https://arxiv.org/pdf/2303.09752.pdf)
- [BERT Loses Patience: Fast and Robust Inference with Early Exit](https://proceedings.neurips.cc/paper/2020/file/d4dd111a4fd973394238aca5c05bebe3-Paper.pdf)
- [Fast Inference from Transformers via Speculative Decoding](https://arxiv.org/pdf/2211.17192.pdf)

## Instruction Tuning & RLHF
- [Training language models to follow instructions with human feedback](https://arxiv.org/pdf/2203.02155.pdf)
- [Finetuned Language Models Are Zero-Shot Learners](https://arxiv.org/pdf/2109.01652.pdf)
- [Multitask Prompted Training Enables Zero-Shot Task Generalization](https://arxiv.org/pdf/2110.08207.pdf)
- [LIMA: Less Is More for Alignment](https://arxiv.org/pdf/2305.11206.pdf)
- [Direct Preference Optimization: Your Language Model is Secretly a Reward Model](https://arxiv.org/pdf/2305.18290.pdf)
- [ZEPHYR: DIRECT DISTILLATION OF LM ALIGNMENT](https://arxiv.org/pdf/2310.16944.pdf)


## LLM Inference
- [Train Short, Test Long: Attention with Linear Biases Enables Input Length Extrapolation](https://arxiv.org/pdf/2108.12409.pdf)
- [RoFormer: Enhanced Transformer with Rotary Position Embedding](https://arxiv.org/pdf/2104.09864.pdf)
- [Fast Transformer Decoding: One Write-Head is All You Need](https://arxiv.org/pdf/1911.02150.pdf)
- [GQA: Training Generalized Multi-Query Transformer Models from Multi-Head CheckpointsFlashAttention: Fast and Memory-Efficient Exact Attention with IO-AwarenessEfficient Memory Management for Large Language Model Serving with PagedAttention](https://arxiv.org/pdf/2305.13245v2.pdf)
- [FlashAttention: Fast and Memory-Efficient Exact Attention with IO-AwarenessEfficient Memory Management for Large Language Model Serving with PagedAttention](https://arxiv.org/pdf/2205.14135.pdf)
- [Efficient Memory Management for Large Language Model Serving with PagedAttention](https://arxiv.org/abs/2309.06180)

## LLM Compression
- [Deep Compression: Compressing Deep Neural Networks with Pruning, Trained Quantization and Huffman Coding](https://arxiv.org/pdf/1510.00149.pdf)
- [8-bit Optimizers via Block-wise Quantization](https://arxiv.org/pdf/2110.02861.pdf)
- [ZeRO: Memory Optimizations Toward Training Trillion Parameter Models](https://arxiv.org/pdf/1910.02054.pdf)
- [QLoRA: Efficient Finetuning of Quantized LLMs](https://arxiv.org/pdf/2305.14314.pdf)
- [BitNet: Scaling 1-bit Transformers for Large Language Models](https://arxiv.org/pdf/2310.11453.pdf)


# (Large) Multimodal Models

## Diffusion Models
- [Maximum Likelihood Training of Score-Based Diffusion Models](https://arxiv.org/pdf/2101.09258.pdf)
- [Denoising Diffusion Implicit Models](https://arxiv.org/pdf/2010.02502.pdf)
- [Denoising Diffusion Probabilistic Models](https://arxiv.org/pdf/2006.11239.pdf)
- [DPM-Solver: A Fast ODE Solver for Diffusion Probabilistic Model Sampling in Around 10 Steps](https://arxiv.org/pdf/2206.00927.pdf)

## Image Generation
- [High-Resolution Image Synthesis with Latent Diffusion Models](https://arxiv.org/pdf/2112.10752.pdf)
- [Photorealistic Text-to-Image Diffusion Models with Deep Language Understanding](https://arxiv.org/pdf/2205.11487.pdf)
- [Scaling Autoregressive Models for Content-Rich Text-to-Image Generation](https://arxiv.org/pdf/2206.10789.pdf)
- [Hierarchical Text-Conditional Image Generation with CLIP Latents](https://arxiv.org/pdf/2204.06125.pdf)
- [PIXART-alpha: Fast Training of Diffusion Transformer for Photorealistic Text-to-Image Synthesis](https://arxiv.org/pdf/2310.00426.pdf)

## Multimodal Model Pre-training	
- [Learning Transferable Visual Models From Natural Language Supervision](https://arxiv.org/pdf/2103.00020.pdf)
- [BLIP: Bootstrapping Language-Image Pre-training for Unified Vision-Language Understanding and Generation](https://arxiv.org/pdf/2201.12086.pdf)
- [CoCa: Contrastive Captioners are Image-Text Foundation Models](https://arxiv.org/pdf/2205.01917.pdf)
- [Scaling Up Visual and Vision-Language Representation Learning With Noisy Text Supervision](https://arxiv.org/pdf/2102.05918.pdf)
- [Sigmoid Loss for Language Image Pre-Training](https://arxiv.org/pdf/2303.15343.pdf)

## Large Multimodal Models
- [Exploring the Limits of Transfer Learning with a Unified Text-to-Text Transformer](https://arxiv.org/pdf/1910.10683.pdf)
- [Language Models are Few-Shot Learners](https://arxiv.org/pdf/2005.14165.pdf)
- [Language Models are Unsupervised Multitask Learners](https://d4mucfpksywv.cloudfront.net/better-language-models/language_models_are_unsupervised_multitask_learners.pdf)
- [Evaluating Large Language Models Trained on Code](https://arxiv.org/pdf/2107.03374.pdf)
- [PaLM: Scaling Language Modeling with Pathways](https://arxiv.org/pdf/2204.02311.pdf)
- [Llama 2: Open Foundation and Fine-Tuned Chat Models](https://arxiv.org/pdf/2307.09288.pdf)

# Augmenting LLM & LMM

## Tool Augmentation
- [Toolformer: Language Models Can Teach Themselves to Use Tools](https://arxiv.org/pdf/2302.04761.pdf)
- [Synergizing Reasoning and Acting in Language Models](https://arxiv.org/pdf/2210.03629.pdf)
- [AgentBench: Evaluating LLMs as Agents](https://arxiv.org/pdf/2308.03688.pdf)
- [ToolLLM: Facilitating Large Language Models to Master 16000+ Real-world APIs](https://arxiv.org/pdf/2307.16789.pdf)
- [ToolkenGPT: Augmenting Frozen Language Models with Massive Tools via Tool Embeddings](https://arxiv.org/pdf/2305.11554.pdf)

## Retrieval Augmentation
- [REALM: Retrieval-Augmented Language Model Pre-Training](https://arxiv.org/pdf/2002.08909.pdf)
- [Retrieval-Augmented Generation for Knowledge-Intensive NLP Tasks](https://arxiv.org/pdf/2005.11401.pdf)
- [Improving language models by retrieving from trillions of tokens](https://arxiv.org/pdf/2112.04426.pdf)
- [Self-RAG: Learning to Retrieve, Generate, and Critique through Self-Reflection](https://arxiv.org/pdf/2310.11511.pdf)
- [REPLUG: Retrieval-Augmented Black-Box Language Models](https://arxiv.org/pdf/2301.12652.pdf)


